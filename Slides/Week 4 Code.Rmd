---
title: "Week 4 Code"
author: "Micaela Wood"
date: "1/24/2022"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
```

First we will practice using the OLS model and calculating the coefficients manually. 

First we will need some data

```{r data for OLS}
set.seed(1)

ols.data <- tibble(
  x = runif(100, 0, 10),
  e = rnorm(100, 0, 15),
  y = 5 + 3*x + e
)

view(ols.data)
```

Let's visualize what this data looks like.

```{r visualize data}
ggplot(ols.data, aes(x = x, y = y))+
  geom_point()
```

First we should solve for the slope coefficient also known as $\hat \beta_2$. 

```{r solve for slope}
#firt find the differences between the variable and its mean
ols.data <- ols.data %>% mutate(
  y.err = y - mean(y), 
  x.err = x - mean(x)
)

#second you need to square the differences in the x's
ols.data <- ols.data %>% mutate(
  x.err.sq = x.err^2
)

#third we need the covariance of y and x
ols.data <- ols.data %>% mutate(
  cov.xy = y.err * x.err
)

#now sum over the x squared error and x, y covariance
cov.sum <- sum(ols.data$cov.xy)
x.err.sq.sum <- sum(ols.data$x.err.sq)

#Finally divide to get b2
b2 <- cov.sum/x.err.sq.sum
b2

```
Notice that this isn't quite 3 but pretty close. That is because we only have 100 observation but have a lot of variance. If we increased the number of observations, the estimate would get closer to 3. 

Now lets calculate the intercept also known as $\hat\beta_1$.

```{r solve for intercept}
#we already have b2 so we just plug it in to the formula

b1 = mean(ols.data$y) - b2 * mean(ols.data$x)
b1
```

This estimate is a little farther away from the true value than the slope estimate. That is alright as we are often not interested in the intercept. This would still get closer to 5 if we increased the observations. 

Now we should replot our graph but include the line that we have estimated. We can do this with `geom_abline`.

```{r plot with estimated coefficients}
ggplot(ols.data, aes(x = x, y = y))+
  geom_point()+
  geom_abline(slope = b2, intercept = b1)
```


What if we want to include control variables. Then this becomes complicated matrix algebra. 

Lets try using the function lm.

```{r easy ols}
lm1 <- lm(data = ols.data, y~x)
```

We can also use this with control variables.

First let's makes a new dataframe

```{r}
n = 100
set.seed(1)

dgp_df <- tibble(
  e = rnorm(n, sd = 30),
  v = rnorm(n, sd = 20),
  x = runif(n, min = 0, max = 10),
  z = rbinom(n, 1, 0.5),
  y = 8 - 3*x + e,
  
)
```

```{r ols with controls}
lm2 <- lm(data = dgp_df, y ~ x + z)
```


